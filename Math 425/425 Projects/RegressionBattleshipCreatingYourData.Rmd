---
title: "Regression Battleship - Creating your Data"
author: "Ben Jacobs"
execute:
  keep-md: TRUE
  df-print: paged
  warning: false
format:
  html:
    theme: cerulean
    code-fold: true
    code-line-numbers: true
date: "2024-06" 
editor: 
  markdown: 
    wrap: 72
---

```{r message=FALSE, warning=FALSE}
library(pander)
library(tidyverse)
library(dplyr)
```

#  {.tabset .tabset-pills}

## Desmos

True Model:

![Model](rb_model.png)

## Code

Use the R-chunks below to create your simulated sample of data from your
true regression model.

```{r, comment=NA}
set.seed(120)
  
n <- 100000


# Actual X values

# for set 1 ----->
x8 <- runif(n,-2.6, 2.6)

# Positive 1 or Negative 1 - With x5 make second line
x9 <- sample(c(-1,1),n,replace=TRUE)

 # Either positive 1 or - 1 - depending on -1 interaction 
x10 <- sample(c(-1,1),n,replace=TRUE)


# 2 term model is base model.

# y3/4 term 1 β3: .25 x * x1 Modifier
x1 <- sample(c(0,1),n,prob=c(.6667, .3333),replace = TRUE)

# y3/4 term 2 β4 : .1666666666 x^3 * x2 Modifier
x2 <- sample(c(0,1),n,prob=c(.6667, .3333),replace = TRUE)

# y5/6 term1 β5: 8 x^-1 * x3 INSERT
x3 <- sample(c(0,1),n,prob=c(.6667, .3333),replace = TRUE)

# y5/6 term 2 β6: .6 x * x4 Modifier
x4 <- sample(c(0,1),n,prob=c(.6667,.3333),replace = TRUE)

# y5/6 term 3 β7: .13333333333 x^3 * x5
x5 <- sample(c(0,1),n,prob=c(.6667,.3333),replace = TRUE)

# These two together give us our x^5 term.
# Case x6 == 1, x7 == 0
# y3/4 term 4 β8: .4 x^5 * x6
x6 <- sample(c(0,1),n,prob=c(.3333,.6667),replace = TRUE)

# Case x6 == 1, x7 == 1
# y3/4 term 4 β9: .2 x^5 * x7
x7 <- sample(c(0,1),n,prob=c(.5,.5),replace = TRUE)


# Include Cases

# x1 - 7 == 0 (Base Model)
# x1,x2,x6, == 1, x5, x7 == 0 (Line 1)
# x3,x4,x5,x6,x7 = 1, x1, x2 = 0 (Line 2)


# Then, create betas, sigma, normal error terms and y
  sigma <- 4 #change to whatever positive number you want
 
  
 ################################
 # You ARE NOT ALLOWED to change this part:
 epsilon_i <- rnorm(n, 0, sigma)
 ################################ 
 
 # An example of how to make Y...
 # y <-  beta0 + beta1*X1 + beta2*X2 + beta3*X4*X2 + epsilon_i
 
 εi <- epsilon_i
 
# Betas
  
βint <- NA # Not using this

# Line pair 1
β1 <- 10 # x
β2 <- -15 # x^3

# Line pair 2
β3 <- -7.5 # x
β4 <- 12.5 # x^3
β8 <- .4 # x^5

# Line pair 3
β5 <- 8  # x^-1 
β6 <- -16 # x
β7 <- 17 #x^3
β8 <- .4 # x^5
β9 <- -.2 # x^5

# For reference

# y1 <- -10 * x -15 * x ^ 3 + εi
# y2 <- +10 * x + 15 * x ^ 3 + εi

# y3 <- -2.5 * x + 2.5 * x ^ 3 -.4* x ^ 5 + εi
# y4 <- +2.5 * x - 2.5 * x ^ 3 + .4* x ^ 5 + εi

# y5 = 8 * x ^ -1 - 6 * x + 2 * x ^ 3 + .2 * x ^ 5 + εi
# y6 = -8 * x ^ -1 + 6 * x - 2 * x ^ 3 - .2 * x ^ 5 + εi

# β3: x1 ----> -.25 x * x1 Modifier
# β4: x2 ----> .1666666666 x^3 * x2 Modifier
# β5: x3 ----> 8 x^-1 * x3 Addition
# β6: x4 ----> -.6 x * x4 Modifier
# β7: x5 ----> .13333333333 x^3 * x5 Modifier
# β8: x6 ----> .4 x^5 * x6 (Case x6 == 1, x7 == 0) Add
# β9: x7 ----> .2 x^5 * x7 (Case x6 == 1, x7 == 1)

# Very Important
x <- x8

y <- (
  (x9 * x10 * (
#   x^-1             x             x^3             x^5
                     β1 * x      + β2 * x^3      +
                     β3 * x * x1 + β4 * x^3 * x2 + β8 * x^5 * x6 +
    β5 * x^-1 * x3 + β6 * x * x4 + β7 * x^3 * x5 + β9 * x^5 * x6 * x7
  ))  + εi
)
 

# This loads your data into a data set:
data <- data.frame(y, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)
 
############### Model Engineering ###############

# Line 1 Bounds - Limit X values for specific cases

lb1.1 <- -1.3716
lb1.2 <- 1.3716

lb2.1 <- -2.8572
lb2.2 <- 2.8572

lb3.1 <- -2.2628
lb3.2 <- -.2992
lb3.3 <- .2992
lb3.4 <- 2.2628


# β3: x1 ----> .25 x * x1 Modifier
# β4: x2 ----> .1666666666 x^3 * x2 Modifier
# β5: x3 ----> 8 x^-1 * x3 INSERT
# β6: x4 ----> .6 x * x4 Modifier
# β7: x5 ----> .13333333333 x^3 * x5
# β8: x6 ----> .4 x^5 * x6 (Case x6 == 1, x7 == 0)
# β9: x7 ----> .2 x^5 * x7 (Case x6 == 1, x7 == 1)

# Include Cases
# x1 - 7 == 0 (Base Model)
# x1,x2,x6, == 1, x5, x7 == 0 (Line 1)
# x3,x4,x5,x6,x7 = 1, x1, x2 = 0 (Line 2)

# Limit Permutation Possibilities
condition1 <- data$x1 == 0 & data$x2 == 0 & data$x3 == 0 & data$x4 == 0 & data$x5 == 0 & data$x6 == 0 & data$x7 == 0
condition2 <- data$x1 == 1 & data$x2 == 1 & data$x3 == 0 & data$x4 == 0 & data$x5 == 0 & data$x6 == 1 & data$x7 == 0
condition3 <- data$x1 == 0 & data$x2 == 0 & data$x3 == 1 & data$x4 == 1 & data$x5 == 1 & data$x6 == 1 & data$x7 == 1

# Limit X values for specific cases
condition4 <- condition1 & ((data$x8 >= lb1.1) & (data$x8 <= lb1.2))
condition5 <- condition2 & ((data$x8 >= lb2.1) & (data$x8 <= lb2.2))
condition6 <- condition3 & (((data$x8 >= lb3.1) & (data$x8 <= lb3.2)) | ((data$x8 >= lb3.3) & (data$x8 <= lb3.4)))

# condition4 <- FALSE
# condition5 <- FALSE
# condition6 <- FALSE

# Combine all conditions
include_all <- condition4 | condition5 | condition6

# Filter rows from the model matrix
rbdata <- data[include_all, ]


############### END ENGINEERING ###############
```

```{r}
write.csv(rbdata,"rbdata.csv",row.names=FALSE)
```

```{r}
test.lm <- lm(y ~ x9:x10:(
                     x8 + I(x8^3) + x8:x1 + I(x8^3):x2 + I(x8^5):x6 + I(x8^-1):x3 + x8:x4 + I(x8^3):x5 + I(x8^5):x6:x7),data=rbdata)

summary(test.lm)
```

```{r}
View(rbdata)
nrow(rbdata)
```

```{r}
plot(y~x8,rbdata)
```

## R Plot

Provide a 2D scatterplot that shows both your *true* model (dashed
lines) and *estimated* model (solid lines) on the same scatterplot. This
should match your Desmos graph.

```{r}
# Reference

test.lm <- lm(y ~ x9:x10:(
                     x8 + I(x8^3) + x8:x1 + I(x8^3):x2 + I(x8^5):x6 + I(x8^-1):x3 + x8:x4 + I(x8^3):x5 + I(x8^5):x6:x7),data=rbdata)

y1 <- function(x){ 10 * x + 15 * x ^ 3}
y2 <- function(x){-10 * x - 15 * x ^ 3}
y3 <- function(x){-2.5 * x + 2.5 * x ^ 3 - 0.4 * x ^ 5}
y4 <- function(x){2.5 * x - 2.5 * x ^ 3 + 0.4 * x ^ 5}
y5 <- function(x){8 * (x ^-1) - 6 * x + 2 * x ^ 3 + 2 * x ^ 5}
y6 <- function(x){-8 * (x ^ -1) + 6 * x - 2 * x ^ 3 - 2 * x ^ 5}

b <- coef(test.lm)

y7 <- function(x,x9=1,x10=1) {
  b[1] +
  x9 * x10 * (
    b[2] * x + b[3] * x^3)
}

y8 <- function(x,x9=1,x10=1,x1=1,x2=1,x3=0,x4=0,x5=0,x6=1,x7=0) {
  b[1] +
  x9 * x10 * (
    b[2] * x + b[3] * x^3 + b[4] * x * x1 + b[5] * x^3 * x2 + b[6] * x^5 * x6 +
    b[7] * x^-1 * x3 + b[8] * x * x4
  )
}

y9 <- function(x,x9=1,x10=1,x1=0,x2=0,x3=1,x4=1,x5=1,x6=1,x7=1) {
  b[1] +
  x9 * x10 * (
    b[2] * x + b[3] * x^3 + b[4] * x * x1 + b[5] * x^3 * x2 + b[6] * x^5 * x6 +
    b[7] * x^-1 * x3 + b[8] * x * x4 + b[9] * x^3 * x5 + b[10] * x^5 * x6 * x7
  )
}

y10 <- function(x,x9=-1,x10=1) {
  b[1] +
  x9 * x10 * (
    b[2] * x + b[3] * x^3
  )
}

y11 <- function(x,x9=-1,x10=1,x1=1,x2=1,x3=0,x4=0,x5=0,x6=1,x7=0) {
  b[1] +
  x9 * x10 * (
    b[2] * x + b[3] * x^3 + b[4] * x * x1 + b[5] * x^3 * x2 + b[6] * x^5 * x6 +
    b[7] * x^-1 * x3 + b[8] * x * x4
  )
}

y12 <- function(x,x9=-1,x10=1,x1=0,x2=0,x3=1,x4=1,x5=1,x6=1,x7=1) {
  b[1] +
  x9 * x10 * (
    b[2] * x + b[3] * x^3 + b[4] * x * x1 + b[5] * x^3 * x2 + b[6] * x^5 * x6 +
    b[7] * x^-1 * x3 + b[8] * x * x4 + b[9] * x^3 * x5 + b[10] * x^5 * x6 * x7
  )
}

ggplot(rbdata,aes(y=y,x=x8,col=interaction(x1,x2,x3,x4,x5,x6,x7,x9*x10))) +
  geom_point(size=1)+ 
  scale_y_continuous(lim=c(-25,+25)) +
  stat_function(fun=y7, col = "gold",linewidth=1.5) +
  stat_function(fun=y8, col = "forestgreen",linewidth=1.5) +
  stat_function(fun=y9, col = "orange",linewidth=1.5) +
  stat_function(fun=y10, col = "purple",linewidth=1.5) +
  stat_function(fun=y11, col = "red",linewidth=1.5) +
  stat_function(fun=y12, col = "cyan",linewidth=1.5)
  
```

```{r}
test.lm <- lm(y ~ x9:x10:(
                     x8 + I(x8^3) + x8:x1 + I(x8^3):x2 + I(x8^5):x6 + I(x8^-1):x3 + x8:x4 + I(x8^3):x5 + I(x8^5):x6:x7),data=rbdata)

```

### Line pair 1

$$
y = 10x - 15x^3
$$

### Line pair 2

$$
y = -7.5x + 12.5x^3 + 0.4x^5
$$

### Line pair 3

$$
y = 8x^{-1} - 16x + 17x^3 + 0.4x^5 - 0.2x^5
$$

## Math Model

Write out your "true" model in mathematical form. Make sure it matches
your code. This could be "painful" if you chose a complicated model.

$$
y = x_9 x_{10} \left(
    10x + (-15)x^3 +
    (-7.5)x x_1 + 12.5x^3 x_2 + 0.4x^5 x_6 +
    8x^{-1} x_3 + (-16)x x_4 + 17x^3 x_5 + (-0.2)x^5 x_6 x_7 +
    \varepsilon_i
\right)
$$ Note - $x9$ and $x10$ would be, in the long equation, distributed to
each term, but for simplicity they will be left factored out.

## Results

Let's compare the three guesses at the true model (from two peers, and
your teacher) to decide who won (i.e., who had the closest guess).

```{r}
# Different Seed for Rbdata2
set.seed(123) 
  
n <- 100000

# Actual X values

# for set 1 ----->
x8 <- runif(n,-2.6, 2.6)

# Positive 1 or Negative 1 - With x5 make second line
x9 <- sample(c(-1,1),n,replace=TRUE)

 # Either positive 1 or - 1 - depending on -1 interaction 
x10 <- sample(c(-1,1),n,replace=TRUE)


# 2 term model is base model.

# y3/4 term 1 β3: .25 x * x1 Modifier
x1 <- sample(c(0,1),n,prob=c(.6667, .3333),replace = TRUE)

# y3/4 term 2 β4 : .1666666666 x^3 * x2 Modifier
x2 <- sample(c(0,1),n,prob=c(.6667, .3333),replace = TRUE)

# y5/6 term1 β5: 8 x^-1 * x3 INSERT
x3 <- sample(c(0,1),n,prob=c(.6667, .3333),replace = TRUE)

# y5/6 term 2 β6: .6 x * x4 Modifier
x4 <- sample(c(0,1),n,prob=c(.6667,.3333),replace = TRUE)

# y5/6 term 3 β7: .13333333333 x^3 * x5
x5 <- sample(c(0,1),n,prob=c(.6667,.3333),replace = TRUE)

# These two together give us our x^5 term.
# Case x6 == 1, x7 == 0
# y3/4 term 4 β8: .4 x^5 * x6
x6 <- sample(c(0,1),n,prob=c(.3333,.6667),replace = TRUE)

# Case x6 == 1, x7 == 1
# y3/4 term 4 β9: .2 x^5 * x7
x7 <- sample(c(0,1),n,prob=c(.5,.5),replace = TRUE)


# Include Cases

# x1 - 7 == 0 (Base Model)
# x1,x2,x6, == 1, x5, x7 == 0 (Line 1)
# x3,x4,x5,x6,x7 = 1, x1, x2 = 0 (Line 2)


# Then, create betas, sigma, normal error terms and y
  sigma <- 4 #change to whatever positive number you want
 
  
 ################################
 # You ARE NOT ALLOWED to change this part:
 epsilon_i <- rnorm(n, 0, sigma)
 ################################ 
 
 # An example of how to make Y...
 # y <-  beta0 + beta1*X1 + beta2*X2 + beta3*X4*X2 + epsilon_i
 
 εi <- epsilon_i
 
# Betas
  
βint <- NA # Not using this

# Line pair 1
β1 <- 10 # x
β2 <- -15 # x^3

# Line pair 2
β3 <- -7.5 # x
β4 <- 12.5 # x^3
β8 <- .4 # x^5

# Line pair 3
β5 <- 8  # x^-1 
β6 <- -16 # x
β7 <- 17 #x^3
β8 <- .4 # x^5
β9 <- -.2 # x^5

# For reference

# y1 <- -10 * x -15 * x ^ 3 + εi
# y2 <- +10 * x + 15 * x ^ 3 + εi

# y3 <- -2.5 * x + 2.5 * x ^ 3 -.4* x ^ 5 + εi
# y4 <- +2.5 * x - 2.5 * x ^ 3 + .4* x ^ 5 + εi

# y5 = 8 * x ^ -1 - 6 * x + 2 * x ^ 3 + .2 * x ^ 5 + εi
# y6 = -8 * x ^ -1 + 6 * x - 2 * x ^ 3 - .2 * x ^ 5 + εi

# β3: x1 ----> -.25 x * x1 Modifier
# β4: x2 ----> .1666666666 x^3 * x2 Modifier
# β5: x3 ----> 8 x^-1 * x3 Addition
# β6: x4 ----> -.6 x * x4 Modifier
# β7: x5 ----> .13333333333 x^3 * x5 Modifier
# β8: x6 ----> .4 x^5 * x6 (Case x6 == 1, x7 == 0) Add
# β9: x7 ----> .2 x^5 * x7 (Case x6 == 1, x7 == 1)

# Very Important
x <- x8

y <- (
  x9 * x10 * (
#   x^-1             x             x^3             x^5
                     β1 * x      + β2 * x^3      +
                     β3 * x * x1 + β4 * x^3 * x2 + β8 * x^5 * x6 +
    β5 * x^-1 * x3 + β6 * x * x4 + β7 * x^3 * x5 + β9 * x^5 * x6 * x7 +
    εi
  )
)
 

# This loads your data into a data set:
data <- data.frame(y, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)
 
############### Model Engineering ###############

# Line 1 Bounds - Limit X values for specific cases

lb1.1 <- -1.3716
lb1.2 <- 1.3716

lb2.1 <- -2.8572
lb2.2 <- 2.8572

lb3.1 <- -2.2628
lb3.2 <- -.2992
lb3.3 <- .2992
lb3.4 <- 2.2628


# β3: x1 ----> .25 x * x1 Modifier
# β4: x2 ----> .1666666666 x^3 * x2 Modifier
# β5: x3 ----> 8 x^-1 * x3 INSERT
# β6: x4 ----> .6 x * x4 Modifier
# β7: x5 ----> .13333333333 x^3 * x5
# β8: x6 ----> .4 x^5 * x6 (Case x6 == 1, x7 == 0)
# β9: x7 ----> .2 x^5 * x7 (Case x6 == 1, x7 == 1)

# Include Cases
# x1 - 7 == 0 (Base Model)
# x1,x2,x6, == 1, x5, x7 == 0 (Line 1)
# x3,x4,x5,x6,x7 = 1, x1, x2 = 0 (Line 2)

# Limit Permutation Possibilities
condition1 <- data$x1 == 0 & data$x2 == 0 & data$x3 == 0 & data$x4 == 0 & data$x5 == 0 & data$x6 == 0 & data$x7 == 0
condition2 <- data$x1 == 1 & data$x2 == 1 & data$x3 == 0 & data$x4 == 0 & data$x5 == 0 & data$x6 == 1 & data$x7 == 0
condition3 <- data$x1 == 0 & data$x2 == 0 & data$x3 == 1 & data$x4 == 1 & data$x5 == 1 & data$x6 == 1 & data$x7 == 1

# Limit X values for specific cases
condition4 <- condition1 & ((data$x8 >= lb1.1) & (data$x8 <= lb1.2))
condition5 <- condition2 & ((data$x8 >= lb2.1) & (data$x8 <= lb2.2))
condition6 <- condition3 & (((data$x8 >= lb3.1) & (data$x8 <= lb3.2)) | ((data$x8 >= lb3.3) & (data$x8 <= lb3.4)))

# condition4 <- FALSE
# condition5 <- FALSE
# condition6 <- FALSE

# Combine all conditions
include_all <- condition4 | condition5 | condition6

# Filter rows from the model matrix
rbdata2 <- data[include_all, ]
```

```{r}
ben.lm <- lm(y ~ x9:x10:(
                     x8 + I(x8^3) + x8:x1 + I(x8^3):x2 + I(x8^5):x6 + I(x8^-1):x3 + x8:x4 + I(x8^3):x5 + I(x8^5):x6:x7),data=rbdata)

benpred <- predict(ben.lm)

zach.lm <- lm(y^2 ~ I(x8^2) + I(x1) + I(x1):I(x8^2) + x7 + x9:x7:x8 + x7:I(x8^2), data = rbdata2)
zachpred <- predict(zach.lm)

sam.lm <- lm(y ~ x8 + I(x8^2) + x4 + x8:x4)
sampred <- predict(sam.lm)

rdat2 <- rbdata2 %>%
  mutate(mycolor = case_when(
     x2==0 &  x3==0 &  x9==-1 &  x10==-1 ~ "Cubic Down",
     x2==0 &  x3==0 &  x9==1 &  x10==1 ~ "Cubic Down",
     x2==0 &  x3==0 &  x9==1 &  x10==-1 ~ "Cubic Up",
     x2==0 &  x3==0 &  x9==-1 &  x10==1 ~ "Cubic Up",
     x2==1 &  x3==0 &  x9==-1 &  x10==-1 ~ "Quintic Up",
     x2==1 &  x3==0 &  x9==1 &  x10==1 ~ "Quintic Up",
     x2==1 &  x3==0 &  x9==1 &  x10==-1 ~ "Quintic Down",
     x2==1 &  x3==0 &  x9==-1 &  x10==1 ~ "Quintic Down",
     x2==0 &  x3==1 &  x9==-1 &  x10==-1  &  x8>0 ~ "Quadratic Up Right",
     x2==0 &  x3==1 &  x9==1 &  x10==1 &  x8>0~ "Quadratic Up Right",
     x2==0 &  x3==1 &  x9==1 &  x10==-1  &  x8<0 ~ "Quadratic Up Left",
     x2==0 &  x3==1 &  x9==-1 &  x10==1 &  x8<0~ "Quadratic Up Left",
     x2==0 &  x3==1 &  x9==-1 &  x10==1 &  x8>0~ "Quadratic Down Right",
     x2==0 &  x3==1 &  x9==1 &  x10==-1 &  x8>0~ "Quadratic Down Right",
     x2==0 &  x3==1 &  x9==-1 &  x10==-1 &  x8<0~ "Quadratic Down Left",
     x2==0 &  x3==1 &  x9==1 &  x10==1 &  x8<0~ "Quadratic Down Left",
  ))

rdat2 <- rdat2 %>%
  mutate( x11 = ifelse(mycolor=="Quadratic Down Right", 1, 0),
          x12 = ifelse(mycolor=="Quadratic Up Right", 1, 0),
          x13 = ifelse(mycolor=="Cubic Down", 1, 0),
          x14 = ifelse(mycolor=="Cubic Up", 1, 0),
          x15 = ifelse(mycolor=="Quintic Down", 1, 0),
          x16 = ifelse(mycolor=="Quintic Up", 1, 0),
          x17 = ifelse(mycolor=="Quadratic Down Left", 1, 0),
          x18 = ifelse(mycolor=="Quadratic Up Left", 1, 0))

saunders.lm <- lm(y ~  x11 +  x11: x8 +   x11:I( x8^2) + #Quadratic Down Right
             x12 +    x12: x8 +  x12:I( x8^2) +  #Quadratic Up Right
             x17 +  x17: x8 +  x17:I( x8^2) +   #Quadratic Down Left
             x18 +  x18: x8 +  x18:I( x8^2) +   #Quadratic Up Left
             x13: x8 +   x13:I( x8^3) +        #Cubic Down
             x14: x8 +   x14:I( x8^3) +        #Cubic Up
             x15: x8 +   x15:I( x8^3) +    x15:I( x8^5) +     #Quintic Down
             x16: x8 +   x16:I( x8^3) +    x16:I( x8^5)     #Quintic Up
            , data=rdat2)

saunderspred <- predict(saunders.lm)

```

### Zach's Guess

```{r}
summary(zach.lm)
```

```{r}
b <- coef(zach.lm)

y1 <- function(x,x9=1,x10=1,x1=0,x2=0,x3=0,x4=0,x5=0,x6=0,x7=0) {
  b[1] + b[2] * x^2 + b[3] * x1 + b[4] * x7 + b[5]* x^2 * x1 + b[6] * x^2 * x7 + b[7] * x7*x*x9 
}
y2 <- function(x,x9=1,x10=1,x1=0,x2=0,x3=0,x4=0,x5=0,x6=1,x7=0) {
  b[1] + b[2] * x^2 + b[3] * x1 + b[4] * x7 + b[5]* x^2 * x1 + b[6] * x^2 * x7 + b[7] * x7*x*x9 
}
y3 <- function(x,x9=1,x10=1,x1=1,x2=1,x3=1,x4=1,x5=1,x6=1,x7=1) {
  b[1] + b[2] * x^2 + b[3] * x1 + b[4] * x7 + b[5]* x^2 * x1 + b[6] * x^2 * x7 + b[7] * x7*x*x9 
}

y4 <- function(x,x9=-1,x10=1,x1=0,x2=0,x3=0,x4=0,x5=0,x6=0,x7=0) {
  b[1] + b[2] * x^2 + b[3] * x1 + b[4] * x7 + b[5]* x^2 * x1 + b[6] * x^2 * x7 + b[7] * x7*x*x9 
}
y5 <- function(x,x9=-1,x10=1,x1=0,x2=0,x3=0,x4=0,x5=0,x6=1,x7=0) {
  b[1] + b[2] * x^2 + b[3] * x1 + b[4] * x7 + b[5]* x^2 * x1 + b[6] * x^2 * x7 + b[7] * x7*x*x9 
}
y6 <- function(x,x9=-1,x10=1,x1=1,x2=1,x3=1,x4=1,x5=1,x6=1,x7=1) {
  b[1] + b[2] * x^2 + b[3] * x1 + b[4] * x7 + b[5]* x^2 * x1 + b[6] * x^2 * x7 + b[7] * x7*x*x9 
}


ggplot(rbdata,aes(y=y,x=x8,col=interaction(x1,x2,x3,x4,x5,x6,x7,x9*x10))) +
  geom_point(size=1)+ 
  scale_y_continuous(lim=c(-25,+25)) +
  stat_function(fun=y1, col = "gold",linewidth=1.5) +
  stat_function(fun=y2, col = "forestgreen",linewidth=1.5) +
  stat_function(fun=y3, col = "orange",linewidth=1.5)+
  stat_function(fun=y4, col = "blue",linewidth=1.5) +
  stat_function(fun=y5, col = "red",linewidth=1.5) +
  stat_function(fun=y6, col = "purple",linewidth=1.5)
```

### Sam's Guess

```{r}
summary(sam.lm)
```

```{r}
y1 <- function(x,x9=1,x10=1,x1=0,x2=0,x3=0,x4=0,x5=0,x6=0,x7=0) {
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x4 + b[5] * x * x4
}
y2 <- function(x,x9=1,x10=1,x1=0,x2=0,x3=0,x4=0,x5=0,x6=1,x7=0) {
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x4 + b[5] * x * x4
}
y3 <- function(x,x9=1,x10=1,x1=1,x2=1,x3=1,x4=1,x5=1,x6=1,x7=1) {
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x4 + b[5] * x * x4
}

y4 <- function(x,x9=-1,x10=1,x1=0,x2=0,x3=0,x4=0,x5=0,x6=0,x7=0) {
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x4 + b[5] * x * x4
}
y5 <- function(x,x9=-1,x10=1,x1=0,x2=0,x3=0,x4=0,x5=0,x6=1,x7=0) {
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x4 + b[5] * x * x4
}
y6 <- function(x,x9=-1,x10=1,x1=1,x2=1,x3=1,x4=1,x5=1,x6=1,x7=1) {
  b[1] + b[2] * x + b[3] * x^2 + b[4] * x4 + b[5] * x * x4
}



ggplot(rbdata,aes(y=y,x=x8,col=interaction(x1,x2,x3,x4,x5,x6,x7,x9*x10))) +
  geom_point(size=1)+ 
  scale_y_continuous(lim=c(-25,+25)) +
  stat_function(fun=y1, col = "gold",linewidth=1.5) +
  stat_function(fun=y2, col = "forestgreen",linewidth=1.5) +
  stat_function(fun=y3, col = "orange",linewidth=1.5)+
  stat_function(fun=y4, col = "blue",linewidth=1.5) +
  stat_function(fun=y5, col = "red",linewidth=1.5) +
  stat_function(fun=y6, col = "purple",linewidth=1.5)
```

### Saunder's Guess

```{r}
summary(saunders.lm)
```

```{r}
palette(c("skyblue","orange","green","purple","steelblue","red","green3","black"))

plot(y ~  x8, data=rdat2, col=as.factor(mycolor))
points(saunders.lm$fit ~  x8, data=rdat2, col=as.factor(mycolor), pch=16, cex=0.5)

b <- coef(saunders.lm)

drawit <- function( x11=0,  x12=0,  x13=0,  x14=0,  x15=0,  x16=0,  x17=0,  x18=0, i=1){
  curve(b[1] + b[2]* x11 + b[3]* x12 + b[4]* x17 + b[5]* x18 + b[6]* x11* x8 + b[7]* x11* x8^2 + b[8]* x8* x12 + b[9]* x8^2* x12 + b[10]* x8* x17 + b[11]* x8^2* x17 + b[12]* x8* x18 + b[13]* x8^2* x18 + b[14]* x8* x13 + b[15]* x13* x8^3 + b[16]* x8* x14 + b[17]* x8^3* x14 + b[18]* x8* x15 + b[19]* x8^3* x15 + b[20]* x15* x8^5 + b[21]* x8* x16 + b[22]* x8^3* x16 + b[23]* x8^5* x16, add=TRUE,  xname="x8", col=palette()[i])  
}

drawit(1,0,0,0,0,0,0,0,4)
drawit(0,1,0,0,0,0,0,0,6)
drawit(0,0,1,0,0,0,0,0,1)
drawit(0,0,0,1,0,0,0,0,2)
drawit(0,0,0,0,1,0,0,0,7)
drawit(0,0,0,0,0,1,0,0,8)
drawit(0,0,0,0,0,0,1,0,3)
drawit(0,0,0,0,0,0,0,1,5)
```

### Predictions

```{r}
summary(benpred)
```

```{r}
summary(zachpred)
```

```{r}
summary(sampred)
```

```{r}
summary(saunderspred)
```

```{r}
# This is a hotfix and is not good
rbdata2 <- rdat2
```

```{r}
# Compute R-squared for each validation

# Get y-hat for each model on new data.
ben.yh <- predict(test.lm, newdata=rbdata2)
zach.yh <- predict(zach.lm, newdata=rbdata2)
sam.yh <- predict(sam.lm, newdata=rbdata2)
saunders.yh <- predict(saunders.lm, newdata=rdat2) # Fixed variable name

# Compute y-bar
ybar <- mean(rbdata2$y) # Yi is given by Ynew from the new sample of data

# Compute SSTO
SSTO <- sum((rbdata2$y - ybar)^2)

# Compute SSE for each model using y - yhat
ben.SSE <- sum((rbdata2$y - ben.yh)^2)
zach.SSE <- sum((rbdata2$y - zach.yh)^2)
sam.SSE <- sum((rbdata2$y - sam.yh)^2)
saunders.SSE <- sum((rbdata2$y - saunders.yh)^2) # Fixed SSE calculation

# Compute R-squared for each
ben.rs <- 1 - ben.SSE / SSTO
zach.rs <- 1 - zach.SSE / SSTO
sam.rs <- 1 - sam.SSE / SSTO
saunders.rs <- 1 - saunders.SSE / SSTO

# Compute adjusted R-squared for each
n <- length(rbdata2$y) # Sample size corrected to rbdata2
ben.p <- length(coef(test.lm)) # Number of parameters in model
zach.p <- length(coef(zach.lm)) # Number of parameters in model
sam.p <- length(coef(sam.lm)) # Number of parameters in model
saunders.p <- length(coef(saunders.lm)) # Number of parameters in model

ben.rsa <- 1 - (n-1)/(n-ben.p)*ben.SSE / SSTO
zach.rsa <- 1 - (n-1)/(n-zach.p)*zach.SSE / SSTO
sam.rsa <- 1 - (n-1)/(n-sam.p)*sam.SSE / SSTO
saunders.rsa <- 1 - (n-1)/(n-saunders.p)*saunders.SSE / SSTO

my_output_table2 <- data.frame(
  Model = c("Ben", "Zach", "Sam", "Saunders"), 
  `Original R2` = c(summary(test.lm)$r.squared, summary(zach.lm)$r.squared, summary(sam.lm)$r.squared, summary(saunders.lm)$r.squared), 
  `Orig. Adj. R-squared` = c(summary(test.lm)$adj.r.squared, summary(zach.lm)$adj.r.squared, summary(sam.lm)$adj.r.squared, summary(saunders.lm)$adj.r.squared), 
  `Validation R-squared` = c(ben.rs, zach.rs, sam.rs, saunders.rs), 
  `Validation Adj. R^2` = c(ben.rsa, zach.rsa, sam.rsa, saunders.rsa)
)

colnames(my_output_table2) <- c("Model", "Original $R^2$", "Original Adj. $R^2$", "Validation $R^2$", "Validation Adj. $R^2$")

knitr::kable(my_output_table2, escape=TRUE, digits=4)

```

Something is wrong with the validation for Zach and Sam. I am not sure
where that problem is arising. The validations should not change that
drastically. It likely originates from the fact that Brother Saunder's
data has a lot of mutations, but those shouldn't effect the
validation...

## Winner:

Definitely Brother Saunders.
