---
title: "Wk 3 Mobius and Quiz"
author: "Ben Jacobs"
execute:
  keep-md: TRUE
  df-print: paged
  warning: false
format:
  html:
    theme: cerulean
    code-fold: true
    code-line-numbers: true
    embed-resources: true
date: "2024-07-02" 
editor: 
  markdown: 
    wrap: 72
---


#Pt 1

```{r}
pacman::p_load(tidyverse,dplyr,pander,car)
```

```{r}
View(Orange)
```

```{r}
o.lm <- lm(circumference ~ age,data=Orange)
plot(circumference ~ age,data=Orange,pch=16,col="red")
abline(o.lm)

summary(o.lm)
```

```{r}
# Calculate RMSE
SSE <- sum(o.lm$residuals^2)

# Calculate MSE
n <- length(Orange$age)
MSE <- SSE / (n - 2)

# Calculate RMSE
rmse <- sqrt(MSE)
rmse
```
```{r}
plot(o.lm,which=1:3)
```
1. Nonconstant Variance
2. Looks good
3. Very funky

```{r}
boxCox(o.lm)
# Lamba = .5 -> sqrt
```
```{r}
o.lm.s <- lm(sqrt(circumference) ~ age,data=Orange)
summary(o.lm.s)

bs <- coef(o.lm.s)
b <- coef(o.lm)

olm <- function(x){b[1] + b[2] * x}
olms <- function(x){(bs[1] + bs[2] * x) ^ 2}

ggplot(Orange, aes(x = age, y = circumference)) +
  geom_point() +
  stat_function(fun = olms, color = "blue") +
  stat_function(fun = olm, color = "red") +
  labs(title = "Circumference vs Age in Orange Trees with Fitted Curve",
       x = "Age",
       y = "Circumference") +
  theme_bw()

plot(o.lm.s, which=1:3)
```


```{r}
# Square prediction to original units
prediction <- predict(o.lm.s,data.frame(age=500))
prediction <- prediction ^2
prediction
```
# QUIZ

```{r}
clm <- lm(dist ~ speed, data=cars)
clms <-  lm(sqrt(dist) ~ speed, data=cars)

b <- coef(clm)
bs <- coef(clms)

clm.f <- function(x){b[1] + b[2] * x}
clms.f <- function(x){(bs[1] + bs[2] * x) ^ 2}

ggplot(cars, aes(x = speed, y = dist)) +
  geom_point() +
  stat_function(fun = clms.f, color = "blue") +
  stat_function(fun = clm.f, color = "red") +
  labs(title = "Circumference vs Age in Orange Trees with Fitted Curve",
       x = "Age",
       y = "Circumference") +
  theme_bw()

boxCox(clm)
```
## Notes

residuals vs fitted plot, not the original plot... make sure you're looking at the original plot.

## Instructions

Use this file to keep a record of your work as you complete the "Skills Quiz: Regression Diagnostics & Transformations" assignment in Canvas.


# PT 2

<!-- Note: The {} after each Problem and Part header allows you to keep track of what work you have completed. Write something like {Done} once you complete each problem and your html file will then show you a nice summary of what you have "done" already. -->

## Problem 1 {}

Open the `Davis` dataset in R, found in `library(car)`. As stated in the help file for this data set, "The subjects were men and women engaged in regular exercise." 

Perform a simple linear regression of the height of the individual based on their weight.

```{r}
library(car)
library(dplyr)
View(Davis)
```


```{r}
dvlm <- lm(height ~ weight, data = Davis)
summary(dvlm)

# b0, b1 * x
# y <- 160.09312 + 0.15086 * x 

mfrow= (c(1:2))

plot(Davis$weight, Davis$height, 
     xlab = "Weight", ylab = "Height",
     main = "Height vs Weight with Regression Line", col="forestgreen", pch=16)

# Add the regression line
abline(dvlm, col = "blue", lwd = 2)

plot(dvlm, which =1:3)
```

```{r}
Davis2 <- Davis %>% slice(-12)
# View(Davis2)

dvlm2 <- lm(height ~ weight, data = Davis2)
summary(dvlm2)

# b0, b1 * x
# y <- 160.09312 + 0.15086 * x 

mfrow= (c(1:2))

plot(Davis2$weight, Davis2$height, 
     xlab = "Weight", ylab = "Height",
     main = "Height vs Weight with Regression Line", col="forestgreen",
     pch=16)

# Add the regression line
abline(dvlm, col = "red", lwd = 2)
abline(dvlm2, col = "blue", lwd = 2)

legend("bottomright", legend = c("lm (with outlier)", "lm (outlier removed)"),
       col = c("red", "blue"), lwd = 2)

plot(dvlm2, which =1:3)
```


### Part (a) {}

Type out the mathematical equation for this regression model and label both $Y$ and $X$ in the equation.

<div class="YourAnswer">

$$
  \underbrace{Y_i}_\text{some label} =  \beta_0\ + Î²_1 X1 +  \epsilon\ where \epsilon \sim \mathcal{N}(0, \sigma^2)
$$

</div>


### Part (b) {}
 
Plot a scatterplot of the data with your regression line overlaid.

<div class="YourAnswer">

```{r}
# Type your code here
```

</div>


### Part (c) {}

Create a residuals vs fitted-values plot for this regression. What does this plot show?

<div class="YourAnswer">

```{r}
# Type your code here...
```

Type your answer here...

</div> 
 
 
### Part (d) {}

State and interpret the slope, y-intercept, and $R^2$ of this regression. Are they meaningful for this data under the current regression?

<div class="YourAnswer">

```{r}
# Type your code here...
```

Type your answer here...

</div>


### Part (e) {}

Run `View(Davis)` in your Console. What do you notice about observation #12 in this data set? 

Perform a second regression for this data with observation #12 removed. Recreate the scatterplot of Part (b) with two regression lines showing this time. The first regression line should include the outlier. The second should exclude the outlier. Include a legend to show which line is which.

<div class="YourAnswer">

```{r}
# Type your code here...
```

</div>


### Part (f) {}

Compute the slope, y-intercept, and $R^2$ value for the regression with the outlier removed. compare the results to the values when the outlier was present.

<div class="YourAnswer">

```{r}
# Type your code here...
```

Type your answer here...

</div>


### Part (g)

Create a residuals vs fitted-values plot for the regression with the outlier removed. How do things look now?

<div class="YourAnswer">

```{r}
# Type your code here...
```

Type your answer here...

</div>


----

## Problem 2 {}

Open the **Prestige** data set found in `library(car)`.

Perform a regression that explains the 1971 average annual **income** from jobs according to their "Pineo-Porter **prestige** score for occupation, from a social survey conducted in the mod-1960's."

```{r}
library(car)
data(Prestige)
View(Prestige)
```

```{r}
plm <- lm(income ~ prestige,data=Prestige)

```


### Part (a) {}

Plot the data and fitted simple linear regression line.

```{r}
plot(income ~ prestige, data= Prestige, col="green", pch=16)
abline(plm, col = "green", pch = 16)
```

</div>


### Part (b) {}

State the estimated values for $\beta_0$, $\beta_1$, and $\sigma$ for this regression. 

<div class="YourAnswer">

```{r}
summary(plm)
sm <- plm$sigma
sm
```

</div>

Sigma is approximated by the residual standard error

### Part (c) {}

Create a residuals vs fitted-values plot and a Q-Q Plot of the residuals for this regression. 

<div class="YourAnswer">

```{r}
plot(plm,which=1:3)
```

</div> 

	

### Part (d) {}

Comment on any difficulties the diagnostic plots in Part (c) reveal about the regression. 

Normality of the error terms is violated, as shown by the points going "out of bounds" in the Q-Q Plot. However, these problems are likely due to the increasing variance of the residuals shown in the residuals vs fitted-values plot. There even may be some problems with linearity of the data because of the three outliers in the top right of the graph pulling the regression line up (shown by the red line going down).


Comment on which estimates of Part (b) are likely effected by these difficulties.

The slope and estimate of the error variance are almost certainly being negatively effected by the increasing variance and three outliers.

<div class="YourAnswer">

Type your answer here...

</div> 
 


----


## Problem 3 {}

Open the **Burt** data set from library(car).

This data set is famous for being fraudulent, or fake. See ?Burt for more details. One of the first indicators that it was fraudulent was revealed by regressing IQbio ~ IQfoster. This regression was just a little too good to be real. (Note that for social science data, like this data, $R^2$ values above 0.3 are impressive. Values above 0.7 are rare.)

```{r}
library(car)
View(Burt)
```


### Part (a) {}

Plot the data and fitted regression line. State the estimated values of $\beta_0$, $\beta_1$, and $\sigma$ as well as the $R^2$ of the regression.

<div class="YourAnswer">

```{r}
blm <- lm(IQbio ~ IQfoster, data=Burt)
plot(IQbio ~ IQfoster, data=Burt, col="blue",pch= 16)
abline(blm)
summary(blm)
```

</div>



### Part (b) {}

Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot for this regression. Are any problems with regression assumption violations visible in these plots?

<div class="YourAnswer">

```{r}
plot(blm,which=1:3)
```

</div>


### Part (c) {}

Comment on what the three diagnostic plots of Part (b) show for the regression. 

<div class="YourAnswer">

They show a fairly nice regression. There may be some slight difficulties with linearity due to the curved red line in the residuals vs fitted-values plot, and points 23 and 24 are somewhat far away from the rest of the data, but otherwise, things are impressively nice.

</div>





----

## Problem 4

Open the **mtcars** data set in R.

Perform a regression of **mpg** explained by the **disp**lacement of the vehicle's engine.

### Part (a) {}

Plot the data and fitted regression line. State the estimated values of $\beta_0$, $\beta_1$, and $\sigma$ as well as the $R^2$ of the regression.

<div class="YourAnswer">

```{r}
mtlm <- lm(mpg ~ disp, data=mtcars)
```

</div>



### Part (b) {}

Create a (1) residuals vs. fitted-values plot, (2) Q-Q Plot of the residuals, and (3) residuals vs. order plot for this regression. Are any problems with regression assumption violations visible in these plots?

<div class="YourAnswer">

```{r}
plot(mpg~disp, data=mtcars)
abline(mtlm)
summary(mtlm)
plot(mtlm, which = 1:3)
```

</div>


### Part (c) {}

Comment on what the three diagnostic plots of Part (b) show for the validity of the values computed in Part (a). 

<div class="YourAnswer">

Not the greatest, not the worst. The residuals near the y intercept are big. The scale location chart has an interesting increasing curve.

</div>


## Problem 5 {}

Open the **Orange** data set found in R.

Perform a regression that explains the **circumference** of the trunk of the orange tree as the tree **age**s.

### Part (a) {}

Plot the data and fitted simple linear regression line.

<div class="YourAnswer">

```{r}
# Type your code here..
```

</div>


### Part (b) {}

State the estimated values for $\beta_0$, $\beta_1$, and $\sigma$ for this regression. 

<div class="YourAnswer">

```{r}
# Type your code here
```

</div>


### Part (c) {}

Create a residuals vs fitted-values plot and a Q-Q Plot of the residuals for this regression. 

<div class="YourAnswer">

```{r}
# Type your code here...
```

</div> 


### Part (d) {}

Comment on any difficulties the diagnostic plots in Part (c) reveal about the regression. 

Comment on which estimates of Part (b) are likely effected by these difficulties.

<div class="YourAnswer">

Type your answer here...

</div> 
 

### Part (e) {}

Perform a Box-Cox analysis of the regression. Which Y-transformation is suggested?

<div class="YourAnswer">

```{r}
# Type your code here...
```

Type your answer here...

</div> 

 
### Part (f) {}

Perform a regression with the transformed y-variable. Plot the regression in the transformed units. Diagnose the fit of the regression on the transformed data.

<div class="YourAnswer">

```{r}
#Type your code here...
```

</div> 


### Part (g) {}

Write out the fitted model for $\hat{Y}_i'$. Then solve the transformed model back into the original units for $\hat{Y}_i$. Then compute the following.

When $X_i = 500$, then $\hat{Y}_i = \ldots$.

<div class="YourAnswer">

$$
  \hat{Y}_i' = ...
$$

$$
  \hat{Y}_i = ...
$$

</div> 


### Part (h)

Plot the data in the original units. Place the transformed line, back in the original units, on this plot. 

<div class="YourAnswer">

```{r}
#Type your code here...
```

</div> 


----








<style>

.YourAnswer {
  color: #317eac;
  padding: 10px;
  border-style: solid;
  border-width: 2px;
  border-color: skyblue4;
  border-radius: 5px;
}

</style>

 
 