---
title: "Skills Quiz: Hypothesis Tests"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
    toc: true
    toc_float: true
---


## Instructions

Use this file to keep a record of your work as you complete the "Skills Quiz: Hypothesis Tests" assignment in Canvas.


----

<!-- Note: The {} after each Problem and Part header allows you to keep track of what work you have completed. Write something like {Done} once you complete each problem and your html file will then show you a nice summary of what you have "done" already. -->


## Problem 1 {}

Install the `Ecdat` library in R: `install.packages("Ecdat")`.

From `library(Ecdat)` open the `Caschool` data set in R.  As stated in the help file for this data set, this data is a collection of measurements on 420 different school districts from California during the 1998-1999 school year.

The school districts in California offer a reduced-price lunch program. This is in a way, a measure of the poverty of the student body of the school district. We will assume that the higher the percentage of participants, the greater the general level of poverty. The question is, does the poverty level (or at least the percentage of participation in the reduced-lunch program) predict how well the student body will perform overall on a standardized test?

`> ?Caschool`

`> View(Caschool)`

```{r}
# install.packages("Ecdat")
library(Ecdat)
library(dplyr)
library(tidyverse)
?Caschool
View(Caschool)
```


### Part (a) {}

Type out the mathematical equation for this regression model and label both $Y$ and $X$ in the equation.

<div class="YourAnswer">

$$
  Yhat = Beta0 + Beta1 * X_i + Epsilon_i
$$

</div>


### Part (b) {}
 
Plot a scatterplot of the data with your regression line overlaid. Write out the fitted regression equation.

<div class="YourAnswer">

```{r}
cas.lm <- lm(testscr ~ mealpct,data=Caschool)
plot(testscr~mealpct, data=Caschool)
abline(cas.lm)

summary(cas.lm)
```

$$
  \hat{Y}_i = Beta0 + Beta1 * X_i
$$

</div>


### Part (c) {}

Report the test statistics and p-values for the following hypotheses.

$$ 
  \begin{array}{l}
    H_0: \beta_0 = 0 \\
    H_a: \beta_0 \neq 0 \\
  \end{array} \quad 
  \begin{array}{l}
    H_0: \beta_1 = 0 \\
    H_a: \beta_1 \neq 0 \\
  \end{array}
$$

<div class="YourAnswer">

```{r}
766.16
35.87
```

The test statistic provides the nmber of standard errors that the estimated value of the parameter stist form the hypothesized value of the true parameter

</div> 
 
 
### Part (d) {}


State the slope, y-intercept, and $R^2$ of this regression. Further, provide 95% confidence intervals for the slope and intercept. Interpret the values.

<div class="YourAnswer">

```{r}

conf_intervals <- confint(cas.lm, level = 0.95)
print(conf_intervals)

```

Type your answer here...

</div>


### Part (e) {}

Create a residuals vs fitted-values plot and Q-Q Plot of the residuals for this regression. What do these plots show?

<div class="YourAnswer">

```{r}
plot(cas.lm, which=1:3)
```

</div>




----

## Problem 2 {}

Open the `Clothing` data set from library(Ecdat).

Although this data is from 1990, it contains two interesting variables (1) the total `tsales` of the clothing stores and (2) the average number of hours worked per employee during the year, `hourspw`. 

`> ?Clothing`

`> View(Clothing)`

```{r}
?Clothing

View(Clothing)
```

```{r}
clothes.lm <- lm(tsales ~ hourspw, data=Clothing)
summary(clothes.lm)
```


### Part (a) {}

Type out the mathematical equation for this regression model and label both $Y$ and $X$ in the equation.

<div class="YourAnswer">

$$
  \underbrace{Y_i}_\text{some label} = ...
$$

</div>


### Part (b) {}
 
Plot a scatterplot of the data with your regression line overlaid. Write out the fitted regression equation.

<div class="YourAnswer">

```{r}
plot(tsales ~ hourspw, data=Clothing)
abline(clothes.lm)

```

$$
  \hat{Y}_i = ...
$$

</div>


### Part (c) {}

Report the test statistics and p-values given by your summary(...) in R for the following hypotheses.

$$ 
  \begin{array}{l}
    H_0: \beta_0 = 0 \\
    H_a: \beta_0 \neq 0 \\
  \end{array} \quad 
  \begin{array}{l}
    H_0: \beta_1 = 0 \\
    H_a: \beta_1 \neq 0 \\
  \end{array}
$$

<div class="YourAnswer">

```{r}
# Type your code here...
```

Type your answer here...

</div> 
 
 
### Part (d) {}

Now, use your own calculations to obtain test statistics and p-values for the following hypotheses.

You may find useful information on how to do this in the "Explanation" tab under "t Tests" from your Math 325 Notebook, Simple Linear Regression page.

$$ 
  \begin{array}{l}
    H_0: \beta_0 = 1500 \\
    H_a: \beta_0 \neq 1500 \\
  \end{array} \quad 
  \begin{array}{l}
    H_0: \beta_1 = 35000 \\
    H_a: \beta_1 \neq 35000 \\
  \end{array}
$$

Note that these hypotheses come from previous knowledge about clothing sales and employee hours. They state that in years past, the average annual sales when no employees worked any hours on average, was 1500. And that as average eployee hours worked increases by 1 hour, the average total annual sales increases by 35,000. The question now, is if the earning pattern has changed from what it used to be.

<div class="YourAnswer">

```{r}
b <- coef(clothes.lm)


model <- lm(tsales ~ hourspw, data = Clothing)

# Coefficients and their standard errors
b <- coef(model)
se <- sqrt(diag(vcov(model)))

# Null values
null_b0 <- 1500
null_b1 <- 35000

# T-statistics
t_b0 <- (b[1] - null_b0) / se[1]
t_b1 <- (b[2] - null_b1) / se[2]

# Display t-statistics
t_b0
t_b1

# Performing the t-tests
p_value_b0 <- 2 * pt(-abs(t_b0), df = model$df.residual)
p_value_b1 <- 2 * pt(-abs(t_b1), df = model$df.residual)

```


```{r}
t_b0
t_b1

# Display p-values
p_value_b0
p_value_b1
```

Type your answer here...

</div>  
 
### Part (e) {}

State the slope, y-intercept, and $R^2$ of this regression. Further, provide 95% confidence intervals for the slope and intercept. Interpret the values.

<div class="YourAnswer">

```{r}
conf_intervals <- confint(clothes.lm, level = 0.95)
print(conf_intervals)
```

Type your answer here...

</div>


### Part (f) {}

Create a residuals vs fitted-values plot and Q-Q Plot of the residuals for this regression. What do these plots show?

<div class="YourAnswer">

variance of error terms = MSE

```{r}
plot(clothes.lm,which=1:3)
```


```{r}
c2 <- Clothing %>% filter(tsales != 5000000)
# View(c2)

c2.lm <- lm(tsales ~ hourspw, data=c2)
summary(c2.lm)
```


```{r}
c2.ssl.lm <- lm(sqrt(sqrt(tsales)) ~ hourspw, data = c2)
summary(clothes.ssl.lm)

# Extract the coefficients
b <- coef(clothes.ssl.lm)

# Define the fitted model function
csslm <- function(x) { b[1] + b[2] * x }

# Plot with ggplot2
library(ggplot2)

ggplot(Clothing, aes(y = sqrt(sqrt(tsales)), x = log(hourspw))) + 
  geom_point() +
  stat_function(fun = csslm, color = "blue") +
  theme_bw() +
  labs(
    title = "Regression of Transformed Sales vs Hours per Week",
    x = "Hours per Week",
    y = "Fourth Root of Sales"
  )
```

```{r}
plot(log(c2$hourspw), sqrt(sqrt(c2$tsales)),
     main = "Regression of Transformed Sales vs Hours per Week",
     xlab = "Hours per Week",
     ylab = "Fourth Root of Sales",
     pch = 19, col = "blue")

# Add the fitted regression line
curve(csslm(x), add = TRUE, col = "red")
```


</div>
 

### Part (g) {}

Do any x-transformations or y-transformations improve the regression? If so, which ones?

<div class="YourAnswer">

```{r}
library(car)
boxCox(clothes.lm)
```

</div>


# Quiz
 
### Q1
Below is the summary output from a simple linear regression performed in R. What is the value of the missing test statistic?
 
```{r}
se <- 1.2679
p <- 0.000555  # This is typically the significance level (alpha)

# Degrees of freedom (df)
df <- 397  # Replace with your actual degrees of freedom

# Calculate the critical t-value
t_value <- qt(1 - p/2, df)  # Use 1 - p/2 for two-tailed test

# Calculate the margin of error (optional)
margin_of_error <- t_value * se

# Print the results
cat("T-value:", t_value, "\n")
cat("Margin of Error:", margin_of_error, "\n")
```
 The t-value from a regression in R is found by taking the "Estimate" and dividing by the "Std. Error". This is because R always uses the null hypothesis that the true parameter (for either the slope or the intercept) is zero. So the t-value is computed by t = (estimate - 0)/std. error = estimate/std. error

In this case: 4.4133 / 1.2679 = 3.480795 which rounds to 3.481.

### Q2
Below is the summary output from a simple linear regression performed in R. Compute the 95% confidence interval for the slope estimate.
```{r}
slope <- 8.5621
se_slope <- .4379
df <- 397  # Replace this with your actual degrees of freedom

# Critical t-value for 95% confidence
t_critical <- qt(1 - 0.05 / 2, df)

# Margin of Error
margin_of_error <- t_critical * se_slope

# Confidence Interval
lower_bound <- slope - margin_of_error
upper_bound <- slope + margin_of_error

# Print the results
cat("Slope Estimate: ", slope, "\n")
cat("95% Confidence Interval: [", lower_bound, ", ", upper_bound, "]\n")
```
Since a 95% confidence interval is obtained by the formula: estimate +/- margin of error, and the margin of error is given by (t*)(std. error) then we have:

qt(1-0.05/2, 397) #gives the critical value for t* = 1.965957

and std. error = 0.4379 #as shown in the summary output

So, 

8.5621 - 1.965957 * 0.4379 #lower bound

8.5621 + 1.965957 * 0.4379 #upper bound

If you used instead

8.5621 +/- 2 * 0.4379 

then you would have still come close to the correct answer, but it would not be as correct as using the actual critical value from the t distribution with 397 degrees of freedom.


### Q3
Suppose a 95% confidence interval for the true regression slope is obtained as (-7.453, -0.947) from a sample of 100 x-y points. What is the p-value for the test of the following hypotheses?

h0: b1 = -10
```{r}
# Given information
conf_interval <- c(-7.453, -0.947)
hypothesized_slope <- -10
n <- 100

# Calculate margin of error
MoE <- (conf_interval[1] - conf_interval[2]) / 2

# Calculate estimate of the slope
SlopeEstimate <- (conf_interval[1] + conf_interval[2]) / 2

# Critical value for margin of error
CriticalValue <- qt(1 - 0.05/2, df = n - 2)

# Calculate standard error of the slope
StandardError <- MoE / CriticalValue

# Calculate t-value
t_value <- (SlopeEstimate - hypothesized_slope) / StandardError

# Calculate two-tailed p-value
p_value <- pt(-abs(t_value), df = n - 2) * 2

# Print results
cat("Margin of Error:", MoE, "\n")
cat("Slope Estimate:", SlopeEstimate, "\n")
cat("Standard Error:", StandardError, "\n")
cat("t-value:", t_value, "\n")
cat("p-value:", p_value, "\n")

```
The margin of error can be found from the confidence interval by using (-7.453 - -0.947)/2 = -3.253. Similarly, the estimate of the slope can be found by finding the middle of the confidence interval (-7.453 + -0.947)/2 = -4.2.

Then, the critical value of the margin of error can be found using qt(1-0.05/2, 100-2) = 1.984467.

This allows us to recover just the standard error of the slope, which is margin of error / critical value = -3.253/1.984467 = -1.639231.

Thus, t = (estimate - hypothesized)/std. error = (-4.2 - -10)/-1.639231 = -3.538244, and the corresponding p-value is clearly very small, but can be computed exactly by pt(-3.538244, 98)*2 which gives p-value = 0.0006175379.

----



<style>

.YourAnswer {
  color: #317eac;
  padding: 10px;
  border-style: solid;
  border-width: 2px;
  border-color: skyblue4;
  border-radius: 5px;
}

</style>


 